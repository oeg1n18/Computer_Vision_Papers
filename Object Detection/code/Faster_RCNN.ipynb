{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91806958",
   "metadata": {},
   "source": [
    "# Faster RCNN\n",
    "\n",
    "\n",
    "The faster Regional CNN is a developemet on the fast RCNN. It introduces the region proposal network that predicts the bounding boxes. This is opposed to the expensive selective search that Fast RCNN uses. The region proposal network uses the features extracted by the convolutional backbone to preict the probability that an object exists within a given anchor. It provides the input for the ROI pool layer. \n",
    "\n",
    "Read more from the paper: https://arxiv.org/abs/1506.01497\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f30839",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a3baec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch_snippets\n",
    "from torch_snippets import *\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "import selectivesearch\n",
    "from IPython.display import clear_output\n",
    "from torch_snippets import Report\n",
    "from torchvision.ops import nms\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "\n",
    "# As the RoIPool layer not implemented on mps \n",
    "# Must use either cuda or cpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355cf041",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "745f7746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimation\n",
    "LEARNING_RATE = 0.005\n",
    "WEIGHT_DECAY = 0.0005\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# Training params \n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57eec90",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e67345fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes(y):\n",
    "    objects = y['annotation']['object']\n",
    "    bboxs = []\n",
    "    for obj in objects:\n",
    "        bbox = list(obj['bndbox'].values())\n",
    "        bbox = [int(i) for i in bbox]\n",
    "        bboxs.append(bbox)\n",
    "    return torch.tensor(bboxs, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11a995bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_labels(y, encode_labels=None):\n",
    "    objects = y['annotation']['object']\n",
    "    class_labels = []\n",
    "    \n",
    "    if encode_labels:\n",
    "        for obj in objects:\n",
    "            label_name = obj['name']\n",
    "            label = encode_labels.index(label_name)\n",
    "            class_labels.append(label)\n",
    "        return torch.tensor(class_labels)\n",
    "    else: \n",
    "        for obj in objects:\n",
    "            label = obj['name']\n",
    "            class_labels.append(str(label))\n",
    "        return class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a3213",
   "metadata": {},
   "source": [
    "# Build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5627fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FRCNNDataset(Dataset):\n",
    "    def __init__(self, root_dir='VOC_data/', set_type='train'):\n",
    "        self.root = root_dir\n",
    "        self.ds = pd.read_csv(root_dir + \"df_\" + set_type + \".csv\")\n",
    "        self.set_type = set_type\n",
    "        self.image_ids = self.ds[\"image_idx\"].unique()\n",
    "        self.resize = torchvision.transforms.Resize((224,224))\n",
    "        \n",
    "        if set_type=='train':\n",
    "            self.raw_ds = VOCDetection(root='data/',year ='2012', \n",
    "                                       image_set=\"train\", download=True, \n",
    "                                       transform=ToTensor())\n",
    "        elif set_type=='test':\n",
    "            self.raw_ds = VOCDetection(root='data/',year ='2012', \n",
    "                                       image_set=\"val\", download=True, \n",
    "                                       transform=ToTensor())\n",
    "        elif set_type=='val':\n",
    "            self.raw_ds = VOCDetection(root='data/',year ='2012', \n",
    "                                       image_set=\"trainval\", download=True, \n",
    "                                       transform=ToTensor())\n",
    "        else:\n",
    "            print(\"set_type must be train, test or trainval\")\n",
    "        \n",
    "            \n",
    "        self.labels = [\"background\", \"person\", \"bird\", \"cat\", \"cow\", \"dog\", \"horse\", \"sheep\", \n",
    "                       \"aeroplane\", \"bicycle\", \"boat\", \"bus\", \"car\", \"motorbike\", \"train\", \n",
    "                       \"bottle\", \"chair\", \"diningtable\", \"pottedplant\", \"sofa\", \"tvmonitor\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image, target = self.raw_ds.__getitem__(image_id)\n",
    "        image = self.resize(image)\n",
    "        gtbbs = get_bounding_boxes(target)\n",
    "        labels = get_class_labels(target,self.labels)\n",
    "        return image, {\"boxes\":gtbbs, \"labels\": labels}      \n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        return tuple(zip(*batch))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c193b921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/VOCtrainval_11-May-2012.tar\n",
      "Extracting data/VOCtrainval_11-May-2012.tar to data/\n",
      "Using downloaded and verified file: data/VOCtrainval_11-May-2012.tar\n",
      "Extracting data/VOCtrainval_11-May-2012.tar to data/\n"
     ]
    }
   ],
   "source": [
    "train_ds = FRCNNDataset(set_type='train')\n",
    "test_ds = FRCNNDataset(set_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91a64d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=train_ds.collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, collate_fn=train_ds.collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "282b49f0-397e-46ab-aa33-dec73bf4193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, targ in train_loader:\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf0620-296b-46a7-a1f2-abb7374f4875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92384dee",
   "metadata": {},
   "source": [
    "# Model Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad2ac059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, len(train_ds.labels))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bf0cbe",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56c1a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(inputs, model, optimizer):\n",
    "    model.train()\n",
    "    input, targets = inputs\n",
    "    input = list(image.to(device) for image in input)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    optimizer.zero_grad()\n",
    "    losses = model(input, targets)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "daa3680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_batch(inputs, model):\n",
    "    model.train()\n",
    "    input, targets = inputs \n",
    "    input = list(image.to(device) for image in input)\n",
    "    targets = [{k: v.to(device) for k,v in t.items} for t in targets]\n",
    "    optimizer.zero_grad()\n",
    "    losses = model(input, targets)\n",
    "    loss = sum(loss for loss in losses.values())\n",
    "    return loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7feb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE,\n",
    "                            momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "log = Report(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b277636d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0.054  trn_loss: 8.706  trn_loc_loss: 0.130  trn_regr_loss: 0.033  trn_objectness_loss: 0.894  trn_rpn_box_reg_loss: 7.649  (20.38s - 1853.40s remaining)))"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m _n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ix, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m----> 4\u001b[0m     loss, losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg \u001b[38;5;241m=\u001b[39m [losses[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_classifier\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m                                                                                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_box_reg\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      7\u001b[0m                                                                                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_objectness\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m                                                                                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_rpn_box_reg\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      9\u001b[0m     pos \u001b[38;5;241m=\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m (ix\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m_n)\n",
      "Cell \u001b[1;32mIn[48], line 7\u001b[0m, in \u001b[0;36mtrain_batch\u001b[1;34m(inputs, model, optimizer)\u001b[0m\n\u001b[0;32m      5\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 7\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m losses\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch_env\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[1;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[0;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch_env\\lib\\site-packages\\torchvision\\models\\detection\\rpn.py:370\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[1;34m(self, images, features, targets)\u001b[0m\n\u001b[0;32m    366\u001b[0m objectness, pred_bbox_deltas \u001b[38;5;241m=\u001b[39m concat_box_prediction_layers(objectness, pred_bbox_deltas)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# apply pred_bbox_deltas to anchors to obtain the decoded proposals\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;66;03m# note that we detach the deltas because Faster R-CNN do not backprop through\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# the proposals\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_coder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bbox_deltas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m proposals \u001b[38;5;241m=\u001b[39m proposals\u001b[38;5;241m.\u001b[39mview(num_images, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    372\u001b[0m boxes, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_proposals(proposals, objectness, images\u001b[38;5;241m.\u001b[39mimage_sizes, num_anchors_per_level)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch_env\\lib\\site-packages\\torchvision\\models\\detection\\_utils.py:178\u001b[0m, in \u001b[0;36mBoxCoder.decode\u001b[1;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box_sum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    177\u001b[0m     rel_codes \u001b[38;5;241m=\u001b[39m rel_codes\u001b[38;5;241m.\u001b[39mreshape(box_sum, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 178\u001b[0m pred_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box_sum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    180\u001b[0m     pred_boxes \u001b[38;5;241m=\u001b[39m pred_boxes\u001b[38;5;241m.\u001b[39mreshape(box_sum, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch_env\\lib\\site-packages\\torchvision\\models\\detection\\_utils.py:216\u001b[0m, in \u001b[0;36mBoxCoder.decode_single\u001b[1;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[0;32m    213\u001b[0m pred_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(dh) \u001b[38;5;241m*\u001b[39m heights[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Distance from center to box's corner.\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m c_to_c_h \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_ctr_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_h\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m pred_h\n\u001b[0;32m    217\u001b[0m c_to_c_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mpred_ctr_x\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mpred_w\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m pred_w\n\u001b[0;32m    219\u001b[0m pred_boxes1 \u001b[38;5;241m=\u001b[39m pred_ctr_x \u001b[38;5;241m-\u001b[39m c_to_c_w\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    _n = len(train_loader)\n",
    "    for ix, inputs in enumerate(train_loader):\n",
    "        loss, losses = train_batch(inputs, model, optimizer)\n",
    "        loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = [losses[k] for k in ['loss_classifier',\n",
    "                                                                                      'loss_box_reg', \n",
    "                                                                                      'loss_objectness',\n",
    "                                                                                      'loss_rpn_box_reg']]\n",
    "        pos = (epoch + (ix+1)/_n)\n",
    "        log.record(pos, trn_loss=loss.item(),\n",
    "                  trn_loc_loss=loc_loss.item(),\n",
    "                  trn_regr_loss=regr_loss.item(),\n",
    "                  trn_objectness_loss=loss_objectness.item(),\n",
    "                  trn_rpn_box_reg_loss=loss_rpn_box_reg.item(), end='\\r')\n",
    "        \n",
    "    _n = len(test_loader)\n",
    "    PATH = \"saved_models/FasterRCNN_EPOCH_\" + str(epoch) + \"_accuracy_\" + \"{0:.4g}\".format(accs.mean())\n",
    "    torch.save(frcnn.state_dict(), PATH)\n",
    "    for ix, inputs in enumerate(test_loader):\n",
    "        loss, losses = train_batch(inputs, model, optimizer)\n",
    "        loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = [losses[k] for k in ['loss_classifier',\n",
    "                                                                                      'loss_box_reg', \n",
    "                                                                                      'loss_objectness',\n",
    "                                                                                      'loss_rpn_box_reg']]\n",
    "        pos = (epoch + (ix+1)/_n)\n",
    "        log.record(pos, val_loss=loss.item(),\n",
    "                  val_loc_loss=loc_loss.item(),\n",
    "                  val_regr_loss=regr_loss.item(),\n",
    "                  val_objectness_loss=loss_objectness.item(),\n",
    "                  val_rpn_box_reg_loss=loss_rpn_box_reg.item(), end='\\r')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac844f",
   "metadata": {},
   "source": [
    "# Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_output(output):\n",
    "    labels = np.array([train_ds.labels[i] for i in output['labels'].cpu().detach().numpy()])\n",
    "    bbs = output['boxes'].cpu().detach().numpy().astype(np.uint16)\n",
    "    confs = output['scores'].cpu().detach().numpy()\n",
    "    ixs = nms(torch.tensor(bbs.astype(np.float32)), torch.tensor(confs), 0.05)\n",
    "    bbs, confs, labels = [tensor[ixs] for tensor in [bbs, confs, labels]]\n",
    "    if len(ixs) == 1:\n",
    "        bbs, confs, labels = [np.array([tensor]) for tensor in [bbs, confs, labels]]\n",
    "    return bbs.tolist(), confs.tolist(), labels.tolist()\n",
    "        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f9ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for ix, (images, targets) in enumerate(train_loader):\n",
    "    if ix==3: break\n",
    "    images = [im for im in images]\n",
    "    outputs = model(images)\n",
    "    print(outputs[0])\n",
    "    for ix, output in enumerate(outputs):\n",
    "        bbs, confs, labels = decode_output(output)\n",
    "        info = [f'{l}@{c:.2f}' for l,c in zip(labels, confs)]\n",
    "        show(images[ix].cpu().permute(1,2,0), bbs=bbs, texts=labels, sz=5, text_sz=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb093a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7bf61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e749b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16c536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931c8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd84d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55860ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
