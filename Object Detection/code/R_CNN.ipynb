{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414e4952",
   "metadata": {},
   "source": [
    "# Regional CNN\n",
    "\n",
    "The regional CNN is an object detection network. The inference pipeline first peroforms selective search on the image. This identifies prospective bounding boxes. Each prospective bounding box is the cropped from the image, resized and send though a backbone convolutional feature extractor. A regression and classification is performed with an MLP from these features. The regression fine tunes the bounding box prediction and the classification identifies which object is contained in the box. \n",
    "\n",
    "Read more at: https://arxiv.org/abs/1311.2524v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4e3a151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">cuda\n",
       "</pre>\n"
      ],
      "text/plain": [
       "cuda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch_snippets\n",
    "from torch_snippets import *\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "import selectivesearch\n",
    "from IPython.display import clear_output\n",
    "from torch_snippets import Report\n",
    "from torchvision.ops import nms\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "# Choose Device\n",
    "if torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f641da",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aae22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Training params \n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Reprocess Dataset\n",
    "REPROCESS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90655ef8",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09573f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_regions(img):\n",
    "    img = img.permute(1, 2, 0)\n",
    "    img_lbl,regions = selectivesearch.selective_search(img, scale=200, min_size=100)\n",
    "    img_area = np.prod(img.shape[:2]) \n",
    "    candidates = [] \n",
    "    for r in regions: \n",
    "        if r['rect'] in candidates: continue\n",
    "        if r['size'] < (0.05*img_area): continue\n",
    "        if r['size'] > (1*img_area): continue\n",
    "        x, y, w, h = r['rect']\n",
    "        candidates.append(list(r['rect']))\n",
    "    candidates = [np.array([x, y, x+w, y+h]) for x, y , w, h in candidates]\n",
    "    return torch.Tensor(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a12a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU(box1, box2, epsilon=1e-5):\n",
    "    x1 = min(box1[0], box2[0])\n",
    "    y1 = min(box1[1], box2[1])\n",
    "    x2 = max(box1[2], box2[2])\n",
    "    y2 = max(box1[3], box2[3])\n",
    "    \n",
    "    if (x2-x1) <= 0 or (y2 - y1) <=0:\n",
    "        return 0.0\n",
    "\n",
    "    union_area = (x2 - x1) * (y2 - y1)\n",
    "    \n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    if (x2 - x1) <=0 or (y2 - y1) <= 0: \n",
    "        return 0.0\n",
    "    \n",
    "    intersection_area = (x2 - x1) * (y2 - y1)\n",
    "    return float(intersection_area / (union_area + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65112895",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def normalize_image(img):\n",
    "    img = normalize(img)\n",
    "    return img.to(device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09aaa37",
   "metadata": {},
   "source": [
    "# Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9113a976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/VOCtrainval_11-May-2012.tar\n",
      "Extracting data/VOCtrainval_11-May-2012.tar to data/\n",
      "Using downloaded and verified file: data/VOCtrainval_11-May-2012.tar\n",
      "Extracting data/VOCtrainval_11-May-2012.tar to data/\n"
     ]
    }
   ],
   "source": [
    "raw_train_dataset = VOCDetection(root='data/',year ='2012', image_set=\"train\", download=True, transform=ToTensor())\n",
    "raw_test_dataset = VOCDetection(root='data/',year ='2012', image_set=\"val\", download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc2d7010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes(y):\n",
    "    objects = y['annotation']['object']\n",
    "    bboxs = []\n",
    "    for obj in objects:\n",
    "        bbox = list(obj['bndbox'].values())\n",
    "        bbox = [int(i) for i in bbox]\n",
    "        bboxs.append(bbox)\n",
    "    return torch.tensor(bboxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28ec8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_labels(y, encode_labels=None):\n",
    "    objects = y['annotation']['object']\n",
    "    class_labels = []\n",
    "    \n",
    "    if encode_labels:\n",
    "        for obj in objects:\n",
    "            label_name = obj['name']\n",
    "            label = encode_labels.index(label_name)\n",
    "            class_labels.append(label)\n",
    "        return torch.tensor(class_labels)\n",
    "    else: \n",
    "        for obj in objects:\n",
    "            label = obj['name']\n",
    "            class_labels.append(str(label))\n",
    "        return class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b87701a-a288-4432-9615-fe526c55c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_img_filepath(y, root_dir=\"data/VOCdevkit\"):\n",
    "    folder = y[\"annotation\"][\"folder\"]\n",
    "    filename = y[\"annotation\"][\"filename\"]\n",
    "    return root_dir + \"/\" + folder + \"/JPEGImages/\" + filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d2ef4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(x, y):\n",
    "    _, H, W = x.shape\n",
    "    candidate_regions = extract_regions(x)\n",
    "    \n",
    "    rois, classes, deltas, crops, paths = [], [], [], [], []\n",
    "    gtbbs = get_bounding_boxes(y)\n",
    "    img_path = get_img_filepath(y)\n",
    "    # calculate IOU for each of the region candidates and the ground truth bounding boxes\n",
    "    ious = np.array([[IOU(candidate, _bb_) for candidate in candidate_regions] for _bb_ in gtbbs]).T\n",
    "    \n",
    "    # calculate candidate class labels\n",
    "    # calculate candidate bounding box offsets\n",
    "    for ix, candidate in enumerate(candidate_regions):\n",
    "        lx, ly, hx, hy = candidate.type(torch.int16)\n",
    "        candidate_ious = ious[ix]\n",
    "        best_iou_at = np.argmax(candidate_ious)\n",
    "        best_iou = candidate_ious[best_iou_at]\n",
    "        best_bb = _lx, _ly, _hx, _hy = gtbbs[best_iou_at]\n",
    "        \n",
    "        #calculate offsets \n",
    "        delta = torch.tensor([_lx-lx, _ly-ly, _hx-hx, _hy-hy])/torch.tensor([W, H, W, H])\n",
    "        \n",
    "        #calculate targets\n",
    "        if best_iou > 0.3:\n",
    "            clss = get_class_labels(y)[best_iou_at]\n",
    "        else: \n",
    "            clss = 'background'\n",
    "            \n",
    "        # calculate candidate offset from image\n",
    "        roi = candidate/torch.tensor([W, H, W, H])\n",
    "        \n",
    "        # calculate the image cropped by the region candidate\n",
    "        crop = x[:, ly:hy, lx:hx]\n",
    "        \n",
    "        crops.append(crop)\n",
    "        classes.append(clss)\n",
    "        deltas.append(delta)\n",
    "        rois.append(roi)\n",
    "        paths.append(img_path)\n",
    "        \n",
    "    paths = np.array(paths)\n",
    "    rois = torch.stack(rois).numpy()\n",
    "    classes = np.array(classes)\n",
    "    deltas = torch.stack(deltas).numpy()\n",
    "    ious = torch.tensor(ious).numpy()\n",
    "\n",
    "    img_dict = {\"paths\":paths, \"rois0\":rois[:, 0], \"rois1\":rois[:, 1], \"rois2\":rois[:, 2], \"rois3\":rois[:, 3],\n",
    "                \"classes\":classes, \"deltas0\":deltas[:, 0], \"deltas1\":deltas[:, 1],  \"deltas2\":deltas[:, 2],  \"deltas3\":deltas[:, 3],\n",
    "               \"gtbbs0\":_lx.item(), \"gtbbs1\":_ly.item(), \"gtbbs2\":_hx.item(), \"gtbbs3\":_hy.item(),}\n",
    "        \n",
    "    img_df = pd.DataFrame(data=img_dict, index=None)\n",
    "    \n",
    "    return img_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09b5705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(raw_dataset, root_dir='VOC_data/', set_type='train', max_datasize=200):\n",
    "    n_images = raw_dataset.__len__()\n",
    "    total_datapoints = 0\n",
    "    x, y = raw_dataset.__getitem__(0)\n",
    "    df = process_img(x, y)\n",
    "    total_datapoints += len(df)\n",
    "    for i in range(1, n_images):\n",
    "        clear_output(wait=True)\n",
    "        print(i, \" Raw images processed producing \", total_datapoints, \" data points ----- \", (total_datapoints*100)/max_datasize, \"% Complete\")\n",
    "        x, y = raw_dataset.__getitem__(i)\n",
    "        new_df = process_img(x,y)\n",
    "        df = pd.concat([df, new_df])\n",
    "        total_datapoints += len(new_df)\n",
    "        if total_datapoints > max_datasize:\n",
    "            break\n",
    "    df.to_csv(\"VOC_data/df_\" + set_type + \".csv\", index=False)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b6c0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if REPROCESS:\n",
    "    preprocess_dataset(raw_train_dataset, set_type='train', max_datasize=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76ff12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if REPROCESS:\n",
    "    preprocess_dataset(raw_test_dataset, set_type='test', max_datasize=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840a2ca",
   "metadata": {},
   "source": [
    "# Prepare Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2ee3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNNDataset(Dataset):\n",
    "    def __init__(self, root_dir='VOC_data/', set_type='train'):\n",
    "        self.root = root_dir\n",
    "        self.ds = pd.read_csv(root_dir + \"df_\" + set_type + \".csv\")\n",
    "            \n",
    "        self.labels = [\"background\", \"person\", \"bird\", \"cat\", \"cow\", \"dog\", \"horse\", \"sheep\", \n",
    "                       \"aeroplane\", \"bicycle\", \"boat\", \"bus\", \"car\", \"motorbike\", \"train\", \n",
    "                       \"bottle\", \"chair\", \"diningtable\", \"pottedplant\", \"sofa\", \"tvmonitor\"]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds)-1\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get the Data point and split into it's labels\n",
    "        data_point = self.ds.iloc[idx].to_numpy()\n",
    "        path = data_point[0]\n",
    "        rois = data_point[1:5]\n",
    "        delta = data_point[6:10]\n",
    "        label = data_point[5]\n",
    "        gtbbs = data_point[10:14]\n",
    "        \n",
    "        # Read in teh image and perform the crop and boudning box transform\n",
    "        img = cv2.cvtColor(cv2.imread(data_point[0]), cv2.COLOR_BGR2RGB)\n",
    "        H, W, _= img.shape\n",
    "        sh = np.array([W, H, W, H])\n",
    "        bbox = (rois*sh).astype(int)\n",
    "        crop = img[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "        return img, crop, bbox, label, delta, gtbbs\n",
    "            \n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        inputs, rois, rixs, labels, deltas, gtbbs = [],[],[],[],[],[]\n",
    "        \n",
    "        # Normalize crop and send labels to devices ready for running\n",
    "        # through the model\n",
    "        for ix in range(len(batch)):\n",
    "            img, crop, bbox, label, delta, gtbb = batch[ix]\n",
    "            crop = torch.tensor(cv2.resize(img, (224,224))).permute(2, 0, 1)\n",
    "            crop = normalize_image(crop/255.)\n",
    "            \n",
    "            inputs.append(crop)\n",
    "            labels.append(self.labels.index(label))\n",
    "            gtbbs.append(gtbb)\n",
    "            deltas.append(torch.tensor(delta.astype(np.float32)))\n",
    "        \n",
    "        inputs = torch.stack(inputs).to(device)\n",
    "        labels = torch.tensor(labels).long().to(device)\n",
    "        deltas = torch.stack(deltas).float().to(device)\n",
    "        return inputs, labels, deltas\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daa2d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = RCNNDataset(set_type='train')\n",
    "test_ds = RCNNDataset(set_type='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028db5f",
   "metadata": {},
   "source": [
    "# Create Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fafe46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=train_ds.collate_fn, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, collate_fn=train_ds.collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca58495",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d29a1aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\miniconda3\\envs\\torch_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\olive\\miniconda3\\envs\\torch_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_backbone = torchvision.models.vgg16(pretrained=True)\n",
    "vgg_backbone.classifier = nn.Sequential()\n",
    "for param in vgg_backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "vgg_backbone.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8701c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        feature_dim = 25088\n",
    "        self.backbone = vgg_backbone\n",
    "        self.cls_score = nn.Linear(feature_dim, 21)\n",
    "        \n",
    "        self.bbox = nn.Sequential(nn.Linear(feature_dim, 512),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(512, 4),\n",
    "                                 nn.Tanh())\n",
    "        \n",
    "        self.cel = nn.CrossEntropyLoss()\n",
    "        self.l1 = nn.L1Loss()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        feat = self.backbone(input)\n",
    "        cls_score = self.cls_score(feat)\n",
    "        bbox = self.bbox(feat)\n",
    "        return cls_score, bbox\n",
    "    \n",
    "    def calc_loss(self, probs, _deltas, labels, deltas):\n",
    "        # Classification loss\n",
    "        detection_loss = self.cel(probs, labels)\n",
    "        \n",
    "        # Regression Loss\n",
    "        ixs, = torch.where(labels !=0)\n",
    "        _deltas = _deltas[ixs]\n",
    "        deltas = deltas[ixs]\n",
    "        self.lmb = 10.0\n",
    "        if len(ixs) > 0:\n",
    "            regression_loss = self.l1(_deltas, deltas)\n",
    "            return detection_loss + self.lmb * regression_loss, detection_loss.detach(), regression_loss.detach()\n",
    "        else:\n",
    "            regression_loss = 0.0\n",
    "            return detection_loss + self.lmb * regression_loss, detection_loss.detach(), regression_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae8a14",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81da33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(_y): \n",
    "    _, preds = _y.max(-1) \n",
    "    return preds\n",
    "\n",
    "def train_batch(inputs, model, optimizer, criterion):\n",
    "    inputs, clss, deltas = inputs\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    _clss, _deltas = model(inputs)\n",
    "    loss, loc_loss, regr_loss = criterion(_clss, _deltas, clss, deltas)\n",
    "    accs = clss == decode(_clss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.detach(), loc_loss, regr_loss, accs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89991688",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_batch(inputs, model, criterion):\n",
    "    input, clss, deltas = inputs\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        _clss,_deltas = model(input)\n",
    "        loss, loc_loss, regr_loss = criterion(_clss, _deltas, clss, deltas)\n",
    "        _, _clss = _clss.max(-1)\n",
    "        accs = clss == _clss\n",
    "    return _clss, _deltas, loss.detach(), loc_loss, regr_loss, accs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de5ba1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0.273  trn_loss: 2.340  trn_loc_loss: 1.388  trn_regr_loss: 0.095  trn_acc: 0.547  (269.20s - 4666.05s remaining))"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m      7\u001b[0m     _n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ix, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      9\u001b[0m         loss, loc_loss, regr_loss, accs \u001b[38;5;241m=\u001b[39m train_batch(inputs, rcnn, optimizer, criterion)\n\u001b[0;32m     10\u001b[0m         pos \u001b[38;5;241m=\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m ((ix\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m_n))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\torch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[18], line 26\u001b[0m, in \u001b[0;36mRCNNDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     23\u001b[0m gtbbs \u001b[38;5;241m=\u001b[39m data_point[\u001b[38;5;241m10\u001b[39m:\u001b[38;5;241m14\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Read in teh image and perform the crop and boudning box transform\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_point\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     27\u001b[0m H, W, _\u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     28\u001b[0m sh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([W, H, W, H])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rcnn = RCNN().to(device)\n",
    "criterion = rcnn.calc_loss\n",
    "optimizer = optim.SGD(rcnn.parameters(), lr=LEARNING_RATE)\n",
    "log = Report(EPOCHS)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    _n = len(train_loader)\n",
    "    for ix, inputs in enumerate(train_loader):\n",
    "        loss, loc_loss, regr_loss, accs = train_batch(inputs, rcnn, optimizer, criterion)\n",
    "        pos = (epoch + ((ix+1)/_n))\n",
    "\n",
    "        log.record(pos, trn_loss=loss.item(), trn_loc_loss=loc_loss, trn_regr_loss=regr_loss, \n",
    "                   trn_acc=accs.mean(), end='\\r')\n",
    "               \n",
    "    _n = len(test_loader)\n",
    "    PATH = \"saved_models/RCNN_EPOCH_\" + str(epoch) + \"_accuracy_\" + \"{0:.4g}\".format(accs.mean())\n",
    "    torch.save(rcnn.state_dict(), PATH)\n",
    "    for ix, inputs in enumerate(test_loader):\n",
    "        _clss, _deltas, loss, loc_loss, regr_loss, accs = validate_batch(inputs, rcnn, criterion)\n",
    "        pos = (epoch + ((ix+1)/_n))\n",
    "        log.record(pos, val_loss=loss.item(), val_loc_loss=loc_loss, \n",
    "            val_regr_loss=regr_loss, \n",
    "            val_acc=accs.mean(), end='\\r')\n",
    "        # plot training and validation metrics\n",
    "        \n",
    "        \n",
    "log.plot_epochs('trn_loss,val_loss'.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc5e3a0",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579b2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98704cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_model(idx, show_output=True):\n",
    "    img, y = raw_train_dataset.__getitem__(idx)\n",
    "    gtbbs = get_bounding_boxes(y)\n",
    "    gt_labels = get_class_labels(y)\n",
    "    candidates = extract_regions(img)\n",
    "    inputs = []\n",
    "    for candidate in candidates:\n",
    "        x,y,X,Y = np.array(candidate).astype(int)\n",
    "        if img.shape[2] == 3:\n",
    "            img = torch.tensor(img)\n",
    "            img = img.permute(2, 0, 1)\n",
    "        else:\n",
    "            img = torch.tensor(img)\n",
    "\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "        crop = cv2.resize(img[y:Y,x:X], (224,224))\n",
    "        crop = torch.tensor(crop).permute(2, 0, 1)\n",
    "        inputs.append(normalize_image(crop/255.)[None])\n",
    "    inputs = torch.cat(inputs).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rcnn.eval()\n",
    "        probs, deltas = rcnn(inputs)\n",
    "        probs = torch.nn.functional.softmax(probs, -1)\n",
    "        confs, clss = torch.max(probs, -1)\n",
    "    candidates = np.array(candidates)\n",
    "    confs, clss, probs, deltas = [tensor.detach().cpu().numpy() for tensor in [confs, clss, probs, deltas]]\n",
    "\n",
    "    ixs = clss!=train_ds.labels.index('background')\n",
    "    confs, clss, probs, deltas, candidates = [tensor[ixs] for tensor in [confs, clss, probs, deltas, candidates]]\n",
    "    bbs = (candidates + deltas).astype(np.uint16)\n",
    "    ixs = nms(torch.tensor(bbs.astype(np.float32)), torch.tensor(confs), 0.05)\n",
    "    confs, clss, probs, deltas, candidates, bbs = [tensor[ixs] for tensor in [confs, clss, probs, deltas, candidates, bbs]]\n",
    "    if len(ixs) == 1:\n",
    "        confs, clss, probs, deltas, candidates, bbs = [tensor[None] for tensor in [confs, clss, probs, deltas, candidates, bbs]]\n",
    "    if len(confs) == 0 and not show_output:\n",
    "        return (0,0,224,224), 'background', 0\n",
    "    if len(confs) > 0:\n",
    "        best_pred = np.argmax(confs)\n",
    "        best_conf = np.max(confs)\n",
    "        best_bb = bbs[best_pred]\n",
    "        x,y,X,Y = best_bb\n",
    "    _, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "    \n",
    "    gtbbs = gtbbs.numpy().tolist()\n",
    "    img = np.ascontiguousarray(img, dtype=np.float32)\n",
    "    torch_snippets.show(img, bbs=gtbbs, texts=gt_labels, ax=ax[0])\n",
    "    ax[0].grid(False)\n",
    "    ax[0].set_title('Original image')\n",
    "    if len(confs) == 0:\n",
    "        ax[1].imshow(img)\n",
    "        ax[1].set_title('No objects')\n",
    "        plt.show()\n",
    "        return\n",
    "    ax[1].set_title(test_ds.labels[clss[best_pred]])\n",
    "    bbs = bbs.tolist()\n",
    "    texts = [train_ds.labels[c] for c in clss.tolist()]\n",
    "    img2 = np.ascontiguousarray(img, dtype=np.float32)\n",
    "    bbx = np.copy(bbs)\n",
    "    torch_snippets.show(img2, bbs=bbx, texts=texts,ax=ax[1])\n",
    "    return (x,y,X,Y), train_ds.labels[clss[best_pred]], best_conf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcad647",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox, label, confidence = test_model(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e224e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for input, bbox, label in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input[0].permute(1,2,0).to(\"cpu\").numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff42c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd463044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4201b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b364699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11de8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
